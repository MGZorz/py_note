# 梯度下降法
## 概念
梯度下降法（Gradient Descent）又称最速下降法（Steepest descent）是一种常用的一阶优化方法，是一种用于求解无约束最优化问题的最常用的方法。它选取适当的初始值$${x^{\left( 0 \right)}}$$，并不断向负梯度方向迭代更新x，实现目标函数的最小化，直到收敛。

## 梯度下降法的直观解释
咱们可以类比于爬山的时候下山，想要从山上的某个位置下山，但是我们不知道山脚的位置，只能一边走一边看，从当前的位置出发，往下（负梯度）走一步，即往最陡峭的方向网下走一步，然后继续求解当前位置的梯度，这样不停地一直往下走，一直走到了“山脚”下，当然我们走到的“山脚”，不一定就是绝对的山脚，也有可能是另一个小土坡。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019082816220085.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjAxODY3MQ==,size_16,color_FFFFFF,t_70)



当目标的函数是凸函数，则可以确保找到全局最优解，否则可能会陷入”陷阱“中（局部最优解）

## 梯度下降法的原理

考虑最优化问题$$\min {}_xf\left( x \right)$$，其中$$f\left( x \right)$$具有一阶连续偏导数。若第$$k$$次迭代值为$${x^{\left( k \right)}}$$，对$$f\left( x \right)$$在$${x^{\left( k \right)}}$$处进行一阶泰勒展开:
$$
f\left( x \right) = f\left( {{x^{\left( k \right)}}} \right) + \left( {x - {x^{\left( k \right)}}} \right)\nabla f\left( {{x^{\left( k \right)}}} \right)
$$
![img](https://img-blog.csdnimg.cn/20181222203042190)

> 凸函数$$f(\theta)$$的某一小段$$[\theta_0,\theta]$$由上图黑色曲线表示，可以利用线性近似的思想求出$$f(\theta)$$的值，如上图红色直线。该直线的斜率等于$$f(\theta)$$在$$\theta_0$$处的导数。则根据直线方程，很容易得到$$f(\theta)$$的近似表达式为：
> $$
> f\left( \theta \right) = f\left( {{\theta _0}} \right) + \left( {\theta - {\theta _0}} \right) \cdot \nabla f\left( {{\theta _0}} \right)
> $$
> 这就是一阶泰勒展开式的推导过程，主要利用的数学思想就是**曲线函数的线性拟合近似**。

其中，$$x - {x^{\left( k \right)}}$$是微小矢量，大小是步长$$\alpha$$，类比于下山过程中的一步。$$\alpha$$是标量，的单位$$x - {x^{\left( k \right)}}$$向量用$$v$$表示，则可以表示为$$x - {x^{\left( k \right)}}$$：
$$
x - {x^{\left( k \right)}} = \alpha v
$$
此时，(1)可以化为：
$$
f\left( x \right) = f\left( {{x^{\left( k \right)}}} \right) + \alpha v\nabla f\left( {{x^{\left( k \right)}}} \right)
$$
我们希望每次迭代，都可以使得$$f(x)$$变小，也就是说往让
$$
\alpha v\nabla f\left( {{x^{\left( k \right)}}} \right)<0
$$
有因为$\alpha$是学习率标量，且都为正值，则$\alpha$可以忽略不计，又因为$v$和$\nabla f\left( {{x^{\left( k \right)}}} \right)$都是向量，根据向量的乘积公式可以将(6)转化为：
$$
v\nabla f\left( {{x^{\left( k \right)}}} \right) = \left\| v \right\| \cdot \left\| {f\left( {{x^{\left( k \right)}}} \right)} \right\|\cos \left( {v,f\left( {{x^{\left( k \right)}}} \right)} \right) < 0
$$
当$v$和$\nabla f\left( {{x^{\left( k \right)}}} \right)$它俩反向的时候，$\cos \left( {v,f\left( {{x^{\left( k \right)}}} \right)} \right) = - 1$,这样就可以使得$v\nabla f\left( {{x^{\left( k \right)}}} \right) $最小了，而且为负，那么即$v$的方向是使局部的目标函数下降最快的方向。得到$v$为：
$$
v = - \frac{{\nabla f\left( {{x^{\left( k \right)}}} \right)}}{{\left\| { \nabla f\left( {{x^{\left( k \right)}}} \right)} \right\|}}
$$
 在将（9）代入（4）中：
$$
x - {x^{\left( k \right)}}=- \alpha\frac{{\nabla f\left( {{x^{\left( k \right)}}} \right)}}{{\left\| { \nabla f\left( {{x^{\left( k \right)}}} \right)} \right\|}}
$$
简单可以看出来$\left\| {f\left( {{x^{\left( k \right)}}} \right)} \right\|$是标量，可以忽略不计（吸收到$\theta$中），那么梯度下降算法的更新表达式为： 
$$
x - {x^{\left( k \right)}} = - \alpha \nabla f\left( {{x^{ \left( k \right)}}} \right)
$$
这就是数学方面的推导公式。


## 梯度下降法的步骤

- 输入：目标函数$f(x)$、梯度函数$\nabla f(x)$，计算精度$\varepsilon$
- 输出：$f(x)$中的极小点$s$



1. 初始化相关参数 ：取初始值${x^{\left( 0 \right)}} \in {R^n}$，迭代次数$k=0$
2. 计算当前位置的目标函数：$f\left( {{x^{\left( 0 \right)}}} \right)$
3. 计算当前位置的目标函数的梯度：$\nabla f\left( {{x^{ \left( k \right)}}} \right)$，如果$\nabla f\left( {{x^{ \left( k \right)}}} \right)<\varepsilon$，那么迭代结束，${x^*} = {x^{\left( k \right)}}$。
4. 更新$x$ ：${x^{\left( {k + 1} \right)}} = {x^{\left( k \right)}} - \alpha \nabla f\left( {{x^{\left( k \right)}}} \right)$，如果$\left\| {{x^{\left( {k + 1} \right)}} - {x^{\left( k \right)}}} \right\| < \varepsilon$或者$\left\| {f\left( {{x^{\left( {k + 1} \right)}}} \right) - f\left( {{x^{\left( k \right)}}} \right)} \right\| < \varepsilon$，则停止迭代，令。否则，${x^*} = {x^{\left( {k + 1} \right)}}$将迭代次数置为$k = k + 1$，转到（4）继续迭代。

在机器学习中，目标函数$f\left( x \right)$实际上就是代价函数$J\left( \theta \right)$。

## 梯度下降种类

### 批量梯度下降法（Batch Gradient Descent，BGD）

批量梯度下降法是梯度下降法最常用的形式，每次更新参数都要使用全部的样本来进行计算。

假设目标函数为：
$$
J\left( \theta \right) = \frac{1}{{2m}}\sum\limits_{i = 1}^m {{{\left( {{y_i} - {h_\theta }\left( {{x_i}} \right)} \right)}^2}}
$$
对（11）求偏导得到：
$$
\frac{{\partial J\left( \theta \right)}}{{\partial {\theta _j}}} = - \frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y_i} - {h_\theta }\left( {{x_i}} \right)} \right){x_{ij}}}
$$
批量梯度下降法的更新公式为：
$$
{\theta ^{\left( {k + 1} \right)}} = {\theta ^{\left( k \right)}} - \alpha \sum\limits_{i = 1}^m {\left( {{y_i} - {h_\theta }\left( {{x_i}} \right)} \right){x_{ij}}}
$$

### 随机梯度下降法 （Stochastic Gradient Descent，SGD）

随机梯度下降法与批量梯度下降法类似，每次更新参数会随机一个样本进行计算

随机梯度下降法的更新公式为：
$$
{\theta ^{\left( {k + 1} \right)}} = {\theta ^{\left( k \right)}} - \alpha \left( {{y_i} - {h_\theta }\left( {{x_i}} \right)} \right){x_{ij}}
$$
**批量梯度下降法和随机梯度下降法的区别是什么？**

- 批量梯度下降法每次使用所有数据来更新参数，训练速度慢；
- 随机梯度下降法每次只使用一个数据来更新参数，训练速度快；但是迭代的方向变化大，不一定每次都是朝着收敛的方向进行迭代的，不能很快的找到最优解。

### 小批量梯度下降法（Mini-Batch Gradient Descent，MBGD）
小批量下降法是对于批量和随机的一个折中，每次更新参数选择**一小部分数据计算**
首先会选择$t$个数据，$1<t<m$

小批量梯度下降法的更新公式为：
$$
{\theta ^{\left( {k + 1} \right)}} = {\theta ^{\left( k \right)}} - \alpha \sum\limits_{i = 1}^t {\left( {{y_i} - {h_\theta }\left( {{x_i}} \right)} \right){x_{ij}}}
$$

### 局部最优解的解决方法
如果目标函数中具有多个局部最优解，不能保证找到全局最优解，为了解决这一问题，常常会采用一下策略跳出局部最优：
- 以多组不同参数值进行初始化，这样有可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果；
-  使用“模拟退火”技术，在每一步都以一定概率接收比当前解更差的结果，有助于跳出局部极小；
-  使用随机梯度下降，最小化每个样本的损失函数，而不是最小化整体的损失函数，虽然不是每次迭代得到的损失函数都朝着收敛的方向， 但是整体的方向是朝着全局最优解的，最终的结果往往是在全局最优解附近。





------------------------------------------------
参考于文章地址：https://blog.csdn.net/pxhdky/article/details/82430196
