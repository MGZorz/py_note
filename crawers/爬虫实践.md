# 爬虫实践

#### 一、简单爬取贴吧信息

##### 思路分析

1.  分为三个部分，分别为获取html(get_html)部分，保存html(save_html)部分以及程勋运行总控（main）。
2.  按照爬取的循序，先获得单页的url，把获取url的步骤放在get_html函数中，答题框架实现。
3.  完善代码，实现可以自定义贴吧名，以及爬取的页码。

##### 代码实现

```python
from urllib.request import  Request,urlopen
# 引入quote实现汉字的转码
from urllib.request import quote

# 获取url函数
def get_html(url):
    headers = {
        'User_Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'
    }
    request = Request(url, headers=headers)
    response = urlopen(request)
    return response.read().decode()

# 保存html函数
def save_html(html, filename):
  # 利用with,实现创建单个文件保存单个html
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(html)

# 主运行函数
def main():
  # 输入贴吧名以及页码
    name = input('请输入想要获取的贴吧名：')
    num = int(input('请输入获取多少页：'))
    #遍历，实现url动态变化
    for i in range(num):
      # 利用'+quote(name)+'给url添加name
        url = 'http://tieba.baidu.com/f?kw='+quote(name)+'&ie=utf-8&pn={}'.format(num*50)
        html = get_html(url)
        filename = '第'+str(i+1)+'页.html'
        save_html(html,filename)

if __name__ == '__main__':
    main()
```

#### 二、简单爬取豆瓣电影排行榜（Ajax）

##### 思路分析

1.  找到准确的Ajax的url
2.  完善整体结构
3.  实现全部爬取

##### 代码实现

```python
from urllib.request import Request,urlopen
# 由于是一直爬取，频率太快，容易造成服务器攻击，加入休眠。
from time import sleep

# 获取到Ajax的准确url地址
url = 'https://movie.douban.com/j/chart/top_list?type=11&interval_id=100%3A90&action=&start=0&limit=1'

headers = {
    'User_Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'
}

i = 0
# 寻找url之间的规律进行修改
base_url = 'https://movie.douban.com/j/chart/top_list?type=11&interval_id=100%3A90&action=&start={}&limit=1'

# 实现死循环，一直爬取服务器中的信息
while True:
    request = Request(base_url.format(i*20), headers=headers)
    response = urlopen(request)
    info = response.read().decode()
		# 结束条件
    if info=='[]':
        break
    else:
        i += 1

    print(info)
    # 休眠3秒
    sleep(3)
```

#### 三、利用ProxyHandler+build_opener实现ip代理

##### 代码实现

```python
# 由于urlopen不支持代理，所以需要额外创建openr函数
from urllib.request import Request, build_opener
# 便捷生成User-Agent
from fake_useragent import UserAgent
# Proxy生成代理哦
from urllib.request import ProxyHandler

url = 'http://httpbin.org/get'
headers = {
    'User_Agent': UserAgent().chrome
}

request = Request(url, headers=headers)
# 创建代理，（）中是json格式的，http协议+ip地址+端口号
handler = ProxyHandler({'http': '125.123.142.10:9999'})
# 根据ip代理创建opener函数
opener = build_opener(handler)
# 请求
response = opener.open(request)
info = response.read().decode()
print(info)
```

#### 四、获取Cookie保存到变量以及Cookie保存文件的读取

##### 思路分析

1.  分为两个函数get_cookie()以及use_cookie()
    1.  get_cookie():
        1.  重点在于创建可用于保存cookie的文件对象，保存cookie以及保存cookie的控制器，重建opener函数
    2.  use_cookie():
        1.  重点在于关联之前的文件对象，提取cookie信息。

##### 代码实现

```python
from urllib.request import Request,build_opener
from fake_useragent import UserAgent
from urllib.request import  HTTPCookieProcessor
from urllib.parse import urlencode
from http.cookiejar import MozillaCookieJar

# 获取Cookie信息
def get_cookie():
		# 请求头
    login_url = 'http://www.sxt.cn/index/login/login'
    form_data = {
        'user':'456465',
        'password':'23156'}
    headers = {'User_Agernt': UserAgent().random}
		# 转换编码
    req = Request(login_url, headers=headers,data=urlencode(form_data).encode())
		# 创建保存可以序列化cookie的文件对象
    cookie_jar = MozillaCookieJar()
    # 构建可保存cookie的控制器
    handler = HTTPCookieProcessor(cookie_jar)
    # 重新创建opener函数
    opener = build_opener(handler)
    # 发生请求  --  登录成功
    resp = opener.open(req)

# 创建cookie.txt文件，把cookie信息保存之中
cookie_jar.save('cookie.txt',ignore_discard=True,ignore_expires=True)

# 使用cookie信息
def use_cookie():
  # 请求头
    info_url = 'http://www.sxt.cn/index/user.html'
    headers={
        'User_Agent':UserAgent().random
    }
    # 转换编码
    req = Request(info_url,headers=headers)
		
    # 把之前创建的Cookie的文件对象关联
    cookie_jar = MozillaCookieJar()
    # 读取cookie.txt中的cookie信息
    cookie_jar.load('cookie.txt',ignore_discard=True,ignore_expires=True)
    # 控制器
    handler = HTTPCookieProcessor(cookie_jar)
    opener = build_opener(handler)

    resp = opener.open(req)
    print(resp.read().decode())

if __name__ == '__main__':
    get_cookie()
    use_cookie()
```

#### 五、爬取糗事百科段子

##### 思路分析

1.  简单爬取html文件
2.  利用正则表达式把其中的段子（想要的信息）提取出来。

##### 代码实现

```python
import requests
from fake_useragent import UserAgent
# 正则截取
import re

url = 'https://www.qiushibaike.com/text/'

headers = {'User-Agent':UserAgent().chrome}
# 相应
resp = requests.get(url,headers=headers)
# 转码
resp.encoding = 'utf-8'
# 正则截取，段子字段，\s表示任意空白字符，也就是目标html中的的空格、换行符等等，*:匹配任意或者多个表达式，.:匹配任意的字符，除了换行符。
# +：匹配一个和多个表达式，总共放在一起的意思是：div+换行换行...+<span>+换行换行...+想要的东西
contents = re.findall(r'<div class="content">\s*<span>\s*(.+)',resp.text)
# for info in contents:
#     print(info)
# 获取的段子生成文本文件
with open('duanzi.txt','w',encoding='utf-8')as f :
    for info in contents:
        f.write(info+'\n\n')
```

#### 六、使用Xpath爬取信息

##### 代码实现

```python
from fake_useragent import UserAgent
import requests
#  etree报错木关系
from lxml import etree

url = 'https://www.qidian.com/rank/yuepiao'
headers = {'User-Agent':UserAgent().chrome}
resp = requests.get(url,headers= headers)
# print(resp.text)
# 解析
e = etree.HTML(resp.text)
# 这里的截取代码是在浏览器的xpath插件中截取的，不过在实际操作中，尽量要在pycharm中寻找相应的位置，因为可能有的页面，通过js渲染和请求返回的不一样。
names = e.xpath('//div[@class="book-mid-info"]/h4/a/text()')
authors = e.xpath('//p[@class="author"]/a[@class="name"]/text()')

# print(names)
# print(authors)

for name,author in zip(names,authors):
    print(name,':',author)
```

#### 七、使用PyQuery爬取信息

##### 代码实现

```python
# 导入pyquery,更名为pq
from pyquery import PyQuery as pq
import requests
from fake_useragent import UserAgent

url = 'https://www.qidian.com/rank/yuepiao'
headers = {'User-Agent':UserAgent().chrome}

resp = requests.get(url,headers=headers)

doc = pq(resp.text)
# 筛选书名和作者,可以借助浏览器的Xpath插件辅助筛选
names= [a.text for a in doc('h4 a')]
# 筛选作者，比较麻烦，因为里面会夹杂很多的其他信息，要遍历偶数才是作者。
author_a =doc('.author a')
# 作者列表
author_list = []

for num in range(len(author_a)):
    if num %2 ==0:
        author_list.append(author_a[num].text)

for name,author in zip(names,author_list):
    print(name,':',author)
```

#### 八、利用JsonPath提取全部地点id和名字

```python
# jsonpath和Xpath非常相似
# 抓起全部地点的名称
from fake_useragent import UserAgent
import requests
from jsonpath import jsonpath
# 制作json对象
from  json import  loads

url = 'https://www.lagou.com/lbs/getAllCitySearchLabels.json'

headers = {'User-Agent':UserAgent().chrome}
resp = requests.get(url,headers = headers)

# print(resp.text)

# 解析,jsonpath(解析的json对象，解析什么（一定要从根目录去写）),利用loads把字符创转换为对象
# ids = jsonpath(loads(resp.text),'$..id')
# names = jsonpath(loads(resp.text),'$..name')

# 优化
ids = jsonpath(resp.json(),'$..id')
names = jsonpath(resp.json(),'$..name')

for id,name in zip(ids,names):
    print(id,':',name)
```

#### 九、爬取电影的相关资讯（电影名、种类、演员表）

##### 思路分析

1.  爬取电影的相关资讯，一定要进入每个电影相应的页面中，那么就应该两部走
    1.  第一步，获取总体页面的html代码（这里可以设置为动态url），解析出每个电影详情的url
    2.  第二部，获取每个电影详情的html代码
2.  进入详情页，获取name /type / actors相关信息
3.  little knowledge  : pycharm快速定位错误位置：f2

##### 代码实现（下面分别使用Xpath，bs4，pyquery，re进行解析）

```python
'''
    爬取猫眼电影的信息
'''

from fake_useragent import UserAgent
import requests
# 为了不那么频繁，访问，设置休眠时间
from time import sleep
# 导入xpath解析html
from lxml import etree
# bs4解析
from bs4 import BeautifulSoup
# pyquey解析
from pyquery import PyQuery as pq
# 正则解析
from re import findall
# 把json写入文件
import json

def get_html(url):
    '''
    :param url: 爬取目标网页
    :return: 接收的html文件
    '''
    headers = {'User-Agent': UserAgent().random}
    resp = requests.get(url, headers=headers)
    return resp.text

def parse_list(html):
    '''
    :param html: 接收到的html文件
    :return: 返回一个包含电影列表的url
    '''
    
    # xpath
    # e = etree.HTML(html)
    # # agency_url = e.xpath('//div[@class="movie-item"]/a/@href')
    # list_url = ['https://maoyan.com{}'.format(agency) for agency in e.xpath('//div[@class="movie-item"]/a/@href')]
    # return list_url

    # bs4
    # soup = BeautifulSoup(html, 'lxml')
    # a_list = soup.select('.channel-detail.movie-item-title > a')
    # list_url = []
    # for a in a_list:
    #     list_url.append(a.get('href'))
    # list_url = ['http://maoyan.com{}'.format(url) for url in list_url]
    # return list_url

    # pyquery
    # doc = pq(html)
    # a_list = doc('.channel-detail.movie-item-title > a')
    # list_url = []
    # for a in a_list:
    #     list_url.append(a.get('href'))
    # list_url = ['http://maoyan.com{}'.format(url) for url in list_url]
    # return list_url

    # re
    a_list = findall(r'<div class="channel-detail movie-item-title".+>\s+<a href="(/films/\d+)"', html)
    list_url = ['http://maoyan.com{}'.format(url) for url in a_list]
    return list_url



def parse_index(html):
    '''
    :param html: 传入有电影信息的html
    :return: 返回提取好的信息
    # name = //div[@class="channel-detail movie-item-title"]/@title
    # authors ://div[@class="info"]/a/text()

    '''
    
		# Xpath
    # e = etree.HTML(html)
    # name = e.xpath('//h3[@class="name"]/text()')[0]
    # type = e.xpath('//div[@class="movie-brief-container"]/ul/li[@class="ellipsis"][1]/text()')[0]
    # actors = e.xpath('//div[@class="info"]/a/text()')
    # actor = del_repeat(actors)
    # return {'name':name,'type':type,'actor':actor}

		# bs4
    # soup = BeautifulSoup(html, 'lxml')
    # name = soup.select('h3.name')[0].text
    # type = soup.select('li.ellipsis')[0].text
    # actors_a = soup.select('li.celebrity.actor > div > a')
    # actors = format_data(actors_a)
    # return {"name": name, "type": type, "actors": actors}

    # pyquery
    # doc = pq(html)
    # name = doc('h3.name')[0].text
    # type = doc('li.ellipsis')[0].text
    # actors_a = doc('li.celebrity.actor > div > a')
    # actors = format_data(actors_a)
    # return {"name": name, "type": type, "actors": actors}

    # re
    name = findall(r'<h3 class="name">(.+)</h3>', html)[0]
    type = findall(r'<li class="ellipsis">(.+)</li>', html)[0]
    actors_a = findall(r'<li class="celebrity actor".+>\s+<a.+\s+<img.+\s+</a>\s+<div.+\s+<a.+>\s+(.+)', html)
    actors = del_repeat(actors_a)
    return {"name": name, "type": type, "actors": actors}


def del_repeat(actors):
    '''
    :param actor:演员表（可能有重复，以及空格）
    :return: 演员表（整理后）
    strip():去除空元素
    set()去除函数
    '''
    actor_set = set()
    for actor in actors:
        actor_set.add(actor.strip())
    return actor_set

# 写入文件没搞明白，-_-!!
def json_str(dict):
    '''
    :param dict:字典文件
    :return: 返回写入成功
    '''
    with open('movie.txt','w',encoding='utf-8')as f :
        f.write(json.dumps(dict))
    return    '写入成功'

def main():
    '''
    主控函数
    '''
    # 不仅仅是爬取一页的信息。动态爬取
    num = int(input('请输入需要爬取的页数：'))
    for page in range(num):
        url = 'https://maoyan.com/films?showType=3&offset={}'.format(page)
        list_html= get_html(url)
        list_url = parse_list(list_html)
        for url in list_url:
            info_html = get_html(url)
            movie = parse_index(info_html)
            print(movie)
            # 休眠3s
            sleep(3)


if __name__ == '__main__':
    main()
```

#### 十、爬取虎牙直播网站的信息（主播名：XXX,观看人数：XXX）

##### 思路分析

​	重点在于，点击下一页，而url并未改变。使用selenium更为方便，不过在获取页面的信息那里最好是使用xpath、re等【九】中用到的方法，因为selenium在页面未完全加载时，会报错终止爬虫，就不好了。

##### 代码实现（selenium/xpath）

```python
from selenium import webdriver
from time import sleep
from lxml import etree

# options = webdriver.ChromeOptions()
# options.add_argument('--headless')
# driver = webdriver.Chrome(chrome_options=options)
driver = webdriver.Chrome()

driver.get('https://www.huya.com/g/lol')


'''使用selenium实现抓取'''
# while True:
#     if driver.find_elements_by_class_name('laypage_next') == -1:
#         break
#     else:
#         driver.find_element_by_class_name('laypage_next').click()
#         sleep(3)
#         names = driver.find_elements_by_class_name('nick')
#         counts = driver.find_elements_by_class_name('js-num')
#         for name, count in zip(names, counts):
#             print(name.text, ':', count.text)


'''使用xpath实现抓取'''
html = driver.page_source
e = etree.HTML(html)
while True:
    if e.xpath('//a[@class="laypage_next"]') == -1:
        break
    else:
        driver.find_element_by_xpath('//a[@class="laypage_next"]').click()
        sleep(3)
        names = e.xpath('//i[@class="nick"]/text()')
        counts = e.xpath('//i[@class="js-num"]/text()')
        for name,count in zip(names,counts):
            print('主播名：',name,',观看人数：',count)
```

#### 十一、使用多线程爬虫

##### 思路分析

1.  导入thread中的Thread，创建一个Spider类（继承于Thread）,重写run函数
2.  导入Queue类，保障线程同步的安全性，也就是说给定一个顺序
3.  利用for遍历，让依次3个线程执行。

##### 代码实现

```python
# 多线程必备模块
from threading import Thread
from queue import Queue
# 爬虫需要的模块
import requests
from fake_useragent import UserAgent
from lxml import etree


# 定义Spilder类，继承于Thread
class Spilder(Thread):
    def __init__(self,url_queue):
        Thread.__init__(self)
        self.url_queue = url_queue

    # 运行函数
    def run(self):
        while not self.url_queue.empty():
            url = self.url_queue.get()
            print(url)

            headers = {'User-Agent':UserAgent().random}
            resp = requests.get(url,headers=headers)
            e = etree.HTML(resp.text)
            contents =[div.xpath('string(.)').strip() for div in e.xpath('//div[@class="content"]')]
            with open('duanzi.txt','w',encoding='utf-8')as f :
                for content in contents:
                    f.write(content+'\n')

if __name__ == '__main__':
    url = 'https://www.qiushibaike.com/text/page/{}/'
    url_queue = Queue()
    num = int(input('请输入要获取的页面：'))
    for page in range(num):
        url_queue.put(url.format(page))
    # 这里是3个线程
    for num in range(3):
        spider = Spilder(url_queue)
        spider.start()
```

#### 十二、使用scrapy框架实现登录功能

##### 思路实现

1.  第一种：已知该网站的form信息的，直接利用FormRequest，发送form请求。
2.  第二种：不知道网站的form表单信息，需要先访问一下主页，获取表单信息，在对其进行修改。
3.  第三种：直接利用cookies登录之前已经登录的界面。

##### 代码实现

```python
第一种：

import scrapy

class Login1Spider(scrapy.Spider):
    name = 'login1'
    allowed_domains = ['baidu.com']
    # start_urls = ['http://baidu.com/']

    def start_requests(self):
        url = '要访问的网站url地址'
        form_data = {
            # 这里是要提交的表单信息
            'username ':'2135616513',
            'password':'456456'
        }
        # 发送请求
        yield  scrapy.FormRequest(url,formdata=form_data,callback=self.parse)
    def parse(self, response):
        print(response.text)
        
第二种：

import scrapy
import re

class Login2Spider(scrapy.Spider):
    name = 'login2'
    allowed_domains = ['ganji.com']
    start_urls = ['https://passport.ganji.com/login.php']

    def parse(self, response):
        # 观察其表单信息，只有验证码，和__hash__的值不能确定，那么就解析他们。
        # 由于hash在页面上找不到，不能用xpath只能用正则表达式去匹配,group(1)是匹配第一个()中的内容的
        hash_code = re.search(r'"__hash__":"(.+)"').group(1)
        # 图片验证码，需要下载下来，然后手动输入（或者用云打码平台），这里用的是手动输入,通过检查找到真实的位置。
        ima_url ='https://passport.ganji.com/ajax.php?dir=captcha&module=login_captcha'

        yield scrapy.Request(ima_url,callback=self.do_formdata,meta={'hash_code':hash_code})

    def do_formdata(self,response):
        hash_code = response.request.meta['hash_code']
        # 把图片验证码打印出来
        with open('yzm.jpg','wb')as f :
            # 这里的response.body就是二进制的代码
            f.write(response.body)
        checkCode = input('请输入验证码：')
        form_data ={
            'username':'213213',
            'password':'654564654',
            'setcookie':'0',
            'checkCode':checkCode,
            'next':'',
            'source':'passport',
            '__hash__':hash_code
        }
        # 再访问一次
        login_url = 'https://passport.ganji.com/login.php'

        yield scrapy.FormRequest(login_url,formdata=form_data,callback=self.get_info)

    def get_info(self,response):
        print(response.text)
        
第三种：

import scrapy

class Login3Spider(scrapy.Spider):
    name = 'login3'
    allowed_domains = ['ganji.com']

    # start_urls = ['http://ganji.com/']

    def start_requests(self):
        url = '想要访问的网址'
        cookies = {}

        str = '_zap=413ad71b-9305-46bf-a73b-fe14f050c006; _xsrf=Gv0TqypkGW9PrWxGQu7HtbqBJmrNQLUm;d_c0="AGDtA3RUpg-PTru2ieZ1msMU8tHVmBE9dRs=|1561640248"; ISSW=1; capsion_ticket="2|1:0|10:1561640260|14:capsion_ticket|44:MTkwNmIxY2I0NTZkNGFlMmIxMTBmN2ZkMjg5ZWJkNGU=|7fe0f3dfd064bf328d419e43a79eb47f68477241db90a65397d122604c989da5"; z_c0="2|1:0|10:1561640282|4:z_c0|92:Mi4xb19zaEF3QUFBQUFBWU8wRGRGU21EeVlBQUFCZ0FsVk5XZ3NDWGdDdUdNZFJuYnB3ZGZXd0J3RFpXcnNxWFZKVmh3|6a91b9af8eee5f1c4494f4ce1f115f10718968f4501ecd2597d61c7d6362caf6"; q_c1=7fd30e9999544362892adf5ece1b25ab|1561640284000|1561640284000; __gads=ID=8ef1044269cd5e91:T=1561683187:S=ALNI_MYz4fsGS6XywwA4upDkK1HlAmnEOA; tshl=film; tst=r; tgw_l7_route=73af20938a97f63d9b695ad561c4c10c'
        # 这个cookie,还是要根据实际进行分解
        for cookie in str.split(';'):
            key, value = cookie.split('=')
            cookies[key] = value

        yield scrapy.Request(url, cookies=cookies, callback=self.parse)

    def parse(self, response):
        print(response.text)
```

#### 十三、爬虫连接数据库（MongoDB、MySQL）

#### 思路分析

1.  首先实现爬虫爬下来的数据保存到MongoDB数据库中，**from pymongo import MongoClient** ,在pipeline文件，MongoDB类中，在开启爬虫时，定义client/数据库名/以及集合名字。然后通过insert把item一一添加进入Mongo，之后关闭client。
2.  连接MySql数据库有一些麻烦，但是总体的步骤和连接Mongo是一样的。**from pymysql import connect**，定义一个MysqlPipeline类，定义client为connet连接数据库的接口，定义cursor光标，利用sql语句添加数据进入数据库，添加之后要**提交**，最后关闭光标+关闭接口。

#### 代码实现(MongoDB)

```python
from pymongo import MongoClient
class PyMongoPipeline(object):
    def open_spider(self, spider):
        self.client = MongoClient()
        self.db = self.client.movie
        self.maoyan = self.db.maoyan

    def process_item(self, item, spider):
        # print(item)
        self.maoyan.insert(item)
        return item

    def close_spider(self, spider):
        self.client.close()
```

#### 代码实现（MySQL）

```python
from pymysql import connect
# 注意mysql是不会自动创造新的数据库的，所以要要先创造
class MysqlPipeline():
    def open_spider(self, spider):
        self.client = connect(host='localhost', port=3306, user='root', password='root', db='movie', charset='utf8')
        self.cursor = self.client.cursor()

    def process_item(self, item, spider):
        sql = 'insert into t_maoyan values(0,%s,%s)'
        self.cursor.execute(sql, [item['name'], item['star']])
        # 注意提交事物
        self.client.commit()
        return item

    def close_spider(self, spider):
        self.cursor.close()
        self.client.close()
```

