# 数据分析

### 什么是数据分析

​		用适当的统计分析方法对收集到的大量数据进行分析，提取出有用的信息，并且对数据加以区分和概括的过程。

> 需要具备的能力：
>
> - 数理知识
> - 数据获取、加工能力
> - 行业相关知识

#### 数据分析步骤

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190722110743332.png)

### 数据加载

​		需要把数据加载到内存中，进行分析，pandas提工了丰富的读取数据的函数，去读位于不同数据源的数据，常用的函数为：`read_csv  / read_table / read_sql / read_json` 。分别为在csv格式文件、table中、数据库中、json格式文件。

#### 常用的参数（上述函数都适用）

- sep/delimiter  ：读取分割符，两个函数效果基本一样。
- header ：`header = None`  文件（csv文件）的第一行不会作为DataFrame的列标签
- names ：设置列索引，参数为列表
- index_col ：设置行索引 
- usecols ：从文件中读取需要的列，传入列表格式

```python
import numpy as np
import pandas as pd

# 使用read_csv()函数
# 利用read_csv 读取csv文件，返回DataFrame对象，默认将第一行作为DataFrame的列。
df = pd.read_csv('data.csv')

# 设置header = None ，csv文件的第一行就不会作为列标签
df = pd.read_csv('data.csv',header = None)

# sep/delimiter , 设置读取分隔符，这俩参数效果类似
df = pd.read_csv('data.csv',header = None ,sep = '-'/delimiter = '-')

# name 设置列索引 传入参数为列表
df = pd.read_csv('data.csv',header = None,names = ['姓名','年龄','身高','体重'])

# index_col  设置行索引  行索引为0开始的整数
df = pd.read_csv('data.csv',header = None ,index_col = 0 )

# usecools   从文件中读取我们所需要的列
df = pd.read_csv('data.csv',header = None,usecools=[1,2])

# 如果数据中心的某列充当了行索引，那么此列的列标签特必须在usecools中设置出来
# 下例中，把第0列的数据作为行索引了。
df = pd.read_csv('data.csv',header= None ,index_col = 0,usecools=[0,1,2])

# 在web中获取数据
# read_table 和 read_csv都可以从网络上获取数据， 和在本地文本获取区别仅仅在于分割符。read_table的分隔符为‘/t’，read_csv的分割符为‘,’
# 这里面分割符是‘,’，所有使用read_csv
df = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')

# 从数据库中获取数据
import pymysql
# 连接数据库
conn = pymysql.connect('localhost','root','root','sxt')
# 将sql,conn 传入read_sql中，读取数据库中数据
df = pd.read_sql('select * from tb_user',conn)

# 从Excel读取数据 sheetname  分表名
df = pd.read_csv('(目录)data/文件名.xslx',sheetname = '分表名')

# 从json中获取数据
df = pd.read_json('data/data.json')
```

### 写入文件（to_csv）

​	 	DataFrame和Series对象的数据通过to_csv方法写入。

#### 常用参数

- index ：是否写入行索引，默认为True
- index_lable ： 索引字段名称
- header ： 是否写入列索引 ，默认为True
- na_rep ： 空值显示
- columns ：写入的字段，默认为全部字段
- sep：分隔符

```python
# 创建一个三行三列的DataFrame对象
df = pd.DataFrame(arrage(1,10),reshape(3,3))

# 通过to_csc将其写入文件中
df.to_csv('data.csv')

# index 默认为True，写入
df.to_csv('data.csv',index = True)

# index_label 设置行索引对象的name信息
df.to_csv('data.csv',index = True,index_label = 'index_name'

# header 设置DataFrame对象列索引写入，True为默认写入,False不写入
df.to_csv('data.csv',header=False)

# 空值默认写入到文件中不显示，na_rep指定文件中空值的显示效果
df.to_csv('data.csv',header=False,na_rep='空')

# columns设置DataFrame对象哪些列写入到文件，默认写入所有的列
df.to_csv('data.csv',header=False,columns=[1,2])

# sep  默认写入文件以逗号为分隔符，sep可以自定义分隔符
df.to_csv('data.csv',header=False,sep='-')
```

### 数据清洗

​		在数据分析中，收集数据无法保证所有的数据一定都是准确以及有效的，所以要把里面的不确定因素清洗掉。分为三方面

#### 缺失值处理

##### 发现缺失值

​		在pandas中将float类型的nan和None看成缺失值。

- info
- isnull
- notnull

> isnull可以any或者all连用

##### 丢弃缺失值

对于缺失值，可通过**dropna**方法进行丢弃处理

- how：指定dropna丢弃缺失值行为，默认为any
- axis：指定丢弃行或者丢弃列，默认丢弃行
- thresh:当非空数值达到该值时，保留数据，否则删除
- inplace：指定是否就地修改，默认为Flase

##### 填充缺失值

对于缺失值，可通过**fillna**方法进行填充

- value：填充所用的值，可以是一个字典，这样为DataFrame不同列指定不同填充值  `字典{列索引:填充值}`
- method：指定上一个有效值填充(ffill)，还是下一个有效值填充(bfill)
  - 前提：前后数据有效并且有关系的数据。
- limit：如果指定method，表示最大连续NaN的填充数量，如果没有指定method，表示最大的NaN填充数量
- inplace：指定是否就地修改，默认为Flase，

```python
# 读取泰坦尼克号数据-titanic_train.csv
df = pd.read_csv('data/titanic_train.csv',header=None)

# 检查缺失值，使用info对整体数据进行查看，显示DataFrame对象每列相关信息
display(df.info())

# isnull发现缺失值
display(df.isnull())
# 检查第10列有没有缺失值
display(df[10].isnull())
# any返回True，说明这一列存在至少一个缺失值
display(df[10].isnull().any())
# all返回True，说明这一列都是空值
display(df[10].isnull().all()) 

# 默认情况下，how=any，只要存在缺失值，直接丢弃行
display(df.dropna())
# how=all,全为缺失值，才丢弃
display(df.dropna(how='all'))

# 存在空值，默认删除行，设置axis=1，丢弃列
display(df.dropna(axis=1))

# how设置any和all都不合适，any条件太宽松，all太严格，自定义非空数据少于某个阈值才删除，thresh
# thresh=11,数组中有12列数据，thresh=11，当数列中的数据个数小于11个，就删除该行)
# 换句话说，当非空的数据个数少于11个，删除数据 
display(df.dropna(thresh=11))

# 使用固定值(均值或中位数)填充所有列缺失值
display(df.fillna(1000))

# 提供字典，为不同列填充不同值。key指定填充对应的索引，value指定填充值
display(df.fillna({5:500,10:1000}))

# 使用method的前提是，前后的数据有紧密的关系、有效的数据
# method，指定前后行的有效数据填充   f:front b:back
display(df.fillna(method='ffill'))
display(df.fillna(method='bfill'))

# limit,单独使用，表示每列总共填充缺失值个数
display(df.fillna(value=1000,limit=1))
# limit参数结合使用，表示最多连续填充个数
display(df.fillna(method='bfill',limit=2))
```

#### 无效值处理

##### 检测无效值

​		没有一个总的方法去检测无效值，只能自己制定，具体情况具体分析，不过可以用describe方法，获得一个描述性的信息。

```python
# 数值信息：若max行的值，大于count行，则该数组有无效值
#           25% 50% 75% 表示从上到下，索引数量的百分数的数值
df = pd.read_csv('data.csv',header=None)
display(df)
display(df.describe())

df = pd.read_csv('data/titanic_train.csv')
display(df.describe())
# describe()方法会默认值找到数值信息的列，想找非数值信息的列，需要单独指定。
# 字符串类型的信息describe函数会得到：总共有多少数据(count) 数据中有哪些是不重复的（unique） 最常出现的数据（top） 出现的次数（freq）
display(df[['Name','Sex','Ticket']].describe())
```

#### 重复值处理

##### 检测重复值

通过duplicated方法发现重复值。该方法返回Series类型对象，值为布尔类型（True和False）表示是否与上一行重复。

- subset：指定依据哪些列判定是否重复，默认为所有列
- keep：指定数据标记为重复(True)的规则。默认为first

##### 删除重复值

​		通过drop_duplicated删除重复值

- subset:指定依据哪些列判断是否重复，默认为所有列
- keep：指定记录删除(或保留)的规则，默认为first
- inplace：指定会否就地删除，默认为False

```python
df = pd.read_csv('data/duplicate.csv',header=None)

# duplicate检查重复值，返回的是Sereis对象，True表示重复，False表示不重复
display(df.duplicated())
# 结合any，判断整个数据是否有重复的数据 
display(df.duplicated().any())

# keep参数：重复N条数据，first：把重复数据中后面的数据标记为True，
#                      last： 把重复数据中前面的数据比较为True；
#                      False，所有的数据都标记为True，且False不需要‘’
display(df.duplicated(keep='last'))

# subset 指定重复规则，默认为：目标行的所有列数据重复才算重复
# 参数为[0,1]表示只检测第0列和第1列的数据是否重复。
display(df.duplicated(subset=[0,1]))

# 删除重复数据
# keep = ‘first’ 删除重复数据中后面的数据，保留前面的数据 ‘last'相反
df.drop_duplicates(keep='first')
```

### 数据过滤

​		就是过滤掉我们不需要的数据，过滤数据的方法有两种：

- 使用布尔数组或者索引数组来过滤
	- 给点一个条件，更具条件生成布尔数组，将得到的布尔数组传回给DataFrame对象中进行过滤。
- 使用DataFrame中的query方法
	- query(‘（条件1）&（条件二）’)

```python
df = pd.read_csv('data/titanic_train.csv')

# 给定一个条件，根据条件生成布尔数组，条件：Survived为1
display(df['Survived']==1)
# 将得到的布尔数组传回给DataFrame对象进行过滤
display(df[df['Survived']==1])
# 注意点，返回的是一个新的DataFrame对象

# 多个条件 用'&'来连接
bArray=(df['Survived']==1)&(df['Pclass']==1)
# display(df[bArray])

# 使用query方法进行过滤
# 1个条件
display(df.query('Survived==1'))
# 多个条件
display(df.query('(Survived==1)&(Pclass==1)'))

# 也可以把条件转化为变量，以1举例，要注意定义变量使用的时候要在前面加@
var = 1
display(df.query('Survived==@var'))
```

### 数据转接

#### 应用与映射

​		 得到的数不符合格式的要求，就需要对其进行应用与映射的数据装换。

##### 应用与映射转换三个函数 

- apply：利用定义的函数来进行元素的处理。
	- Series
	- DataFrame中apply方法 
		- 参数中多一个axis参数指定行列方向，但是不同的是这里的axis =0 表示每一列的数据，axis=1 表示每一行的数据。
- map ：对当前的Series对象的值进行映射装换，参数可以是Series、字典或者函数，注意的是参数不能为DataFrame对象。
- applymap：对DataFrame对象进行操作，参数为函数。

###### apply函数

```python
# apply函数可以对Series,DataFrame进行应用和映射的数据转换
# 处理Series
# apply方法的实现流程：会把数组的元素依次传入之前定义好的函数中，利用函数对元素进行处理，在将处理的结果返回。
def handle(x):
    print('in handle',x)
    return x**2

# 数组对象s ,转换为各元素的平方
s = pd.Series([1,2,3,4])
s = s.apply(handle)  # apply传入的参数是函数名
# 也可以同lambda表达式，给apply传入简单一行的匿名方法
s = s.apply(lambda x: x**2)
# 也可以返回多个值，并且是以元组的方法显示(handle修改：return x**2,x**3)
display(s)

# 处理DataFrame
# 目的：每行的female 》 女，male》男
def handle(row):
    if row.loc['Sex']=='female':
        row.loc['Sex']='女'
    else:
        row.loc['Sex'] = '男'
        
df = pd.read_csv('data/titanic_train.csv')
# 参数中多一个axis参数指定行列方向，但是不同的是这里的axis =0 表示每一列的数据，axis=1 表示每一行的数据。
df = df.apply(handle,axis=1)
# 转化英文变中文
```

###### map函数

```python
# map函数适用于Series对象
# map(函数名)，和apply函数没有区别
s = pd.Series([1,2,3,4])
s = s.map/apply(lambda x:x**2)

# 传入Series
map_series = pd.Series([11,12,13,14])
# 映射规则：
# 		取s中的一个值，和map_series索引进行匹配，匹配上，找到对应的map_series中值映射给s，若想让其对应匹配，则需要指定索引，index = [1,2,3,4]
display(s.map(map_series))

# 传入字典
# 映射规则：
# 		取s中的一个值，和map_dict中的key值进行匹配，匹配上，找到key对应的value映射给s。
s = pd.Series([1,2,3,4])
map_dict={1:11,2:12,3:13,4:14}
```
###### applymap函数

```python
# applymap对DataFrame的元素转换映射操作。
df = pd.read_csv('data/titanic_train.csv')
# 查看全部字段名，同时也查看他们全部的数据类型
display(df.head(),df.info())
# 仅仅对于数据类型为int和float类型的字段操作
display(df[['Age','Fare']])
df = df[['Age','Fare']].applymap(lambda x:x+10000)
```

##### 映射三函数的总结

###### 	按照所针对的对象进行分类

- Series对象---》一个个元素构成
	- 对元素进行转换操作得到一个新的元素类 --》apply / map
		- 参数为函数  匿名函数
		- 运算规则
			- 依次将元素传递给参数函数
			- 函数中对数据进行转换
			- 将换后的数据返回
	- 直接映射得到新的元素---》map
		- Series(label–value)-map_series 
			-  取s中的一个值，和map_series索引进行匹配，匹配上，找到对应的map_series中值映射给s，若想让其对应匹配，则需要指定索引，index = [1,2,3,4]
		- 字典(key-value)-map_dict 
			- 取s中的一个值，和map_dict中的key值进行匹配，匹配上，找到key对应的value映射给s。
- DataFrame对象
	- 数据少--》元素
		- 操作转换得到新的元素--》applymap
			- applymap的参数为`函数`，DataFrame的每一个元素都调用一次该函数，并将元素传递给该函数进行处理，并返回。
	- 数据多--》行或者列（axis : 0对应列  1 对应行）
		- 依次将行或者列的数据传递给函数参数
		- 函数中对行或者列数据进行转换
		- 将换后行或者列数据返回

#### 替换

​	Series和DataFrame都可以通过replace方法实现元素值的替换。

- to_replace：被替换值，支持单一替换、列表、字典和正则表达式
- regex：参数决定是否使用正则表达式，默认为False

```python
# replace 元素替换
df = pd.read_csv('data/bikes.csv')

# replace 单一值替换  对DataFrame 指定单一的值进行替换
df.replace('4/16/2010','2010-4-16')
# 对Series
s = df['THEFT_DATE'].replace('4/16/2010','2010-4-16')

# replace支持列表，将列表元素替换为value指定的值
s = df['THEFT_DATE'].replace(['4/16/2010','4/24/2010'],'2010-4-16')

# replace 支持列表多个值，每个替换为不同的值，列表执行对应位置替换
s = df['THEFT_DATE'].replace(['4/16/2010','4/24/2010'],['2010-4-16','2010-4-24'])

# replace支持字典方式将多个key指定要替换的值，value为要替换为什么值
s = df['THEFT_DATE'].replace({'4/16/2010':'2010-4-16','4/24/2010':'2010-4-24'})

# replace支持正则表达式，在使用正则表达式之前，要加上参数regex = True
# 相当于告诉内存，前面那个字符串为正则表达
s = df['THEFT_DATE'].replace('4/16/[0-9]{4}','2010',regex=True)

# replace 同时可以使用map或者applymap来实现，需要之前定义一个函数
def m(item):
    if item=='4/16/2010':
        return '2010-4-16'
    return item
s = df['THEFT_DATE'].map(m)
```

#### 字符串向量化运算

​	针对于Series对象而言，其中含有str，能够对字符串进行向量级运算，从而数据进行转换。

```python
s= pd.Series(['a','b','c'])
# 把其全部转化为大写字母
display(s.str.upper())
# 验证是否包含b
display(s.str.contains('b'))

# 对数据进行过滤
df = pd.read_csv('data/earthquake_week.csv')
# 用str的endswith方法，判断字符串最后的字符是不是CA，是,True 不是，Flase
barray = df['place'].str.endswith('CA')

# split方法对数据进行拆分
# expand=False，用一个列表存放拆分后的数据，expand=True，拆分为多列  按照，拆分
display(df['place'].str.split(',',expand=True))
```

### 数据合并

#### concat

​	通过pandas的concat方法，对DataFrame或者Series类型的进行连接操作。连接时，**根据索引对齐**。

##### 常用参数说明

- axis：指定连接轴，默认为0 （横轴）
- join：指定连接方式，默认为外连接，参数值为outer：并集，inner：交集
- keys：用来区分不同的数据组
- join_axes：指定连接结果集中保留的索引
- ignore_index：忽略原来连接索引，创建新的整数序列索引，默认为False

```python
df1 = pd.DataFrame(np.arange(1,10).reshape(3,3))
df2 = pd.DataFrame(np.arange(10,19).reshape(3,3),columns=[1,2,3])
# 进行concat连接，根据索引（默认索引）对齐合并，如果不对齐，产生则会空值
display(pd.concat((df1,df2)))

# axis指定数据连接方向，axis=0 竖直，列索引对齐；axis=1水平方向，行索引对齐
display(pd.concat((df1,df2),axis=1))

# join参数指定连接方式，inner显示公共内容，outer显示所有的内容
display(pd.concat((df1,df2),join='outer'))

# keys观察新的DataFrame中数据的来源
display(pd.concat((df1,df2),keys=['df1','df2']))

# 通过join_axes指定要保留的索引(指定保留df2的索引)
display(pd.concat((df1,df2),join_axes=[df2.columns]))

# ignore_index=True 忽略之前的索引，重新创建连续的索引
display(pd.concat((df1,df2),ignore_index=True))
```

#### append

​		Series或DataFrame的append方法可以对行进行连接操作

#### merge

​		通过pandas或者DataFrame的merge方法，可以进行两个DataFrame对象的连接，这种连接也类似于SQL中对两张表进行的join连接。

##### 常用参数

- how：指定连接方式，可以是inner，outer，left，right，默认为inner。
- on：指定连接使用的列（该列必须是同时出现在要连接的两个DataFrame中的），默认使用两个DataFrame所有同名的列相连接。
- left_on / right_on：指定左右DataFrame连接所使用的列
- left_index / right_index：是否将左边（右边）DataFrame的索引作为连接列，默认为False。
- suffixes：当两个DataFrame列名相同时，指定每个列名的后缀（用来区分），默认为`_x和_y`。

```python
# 数据库内连接步骤为：1、数据组合，2、根据等值条件查询
# DataFrame merge 连接步骤和数据库内连接的步骤一样。
# 2个DataFrame连接时候，默认将所有的同名列索引作为连接条件，且为等值连接。

# A.列名都相同，且列值也相同。		结果：他本身。
df1 = pd.DataFrame(np.array([[1,2],[3,4]]))
df2 = pd.DataFrame(np.array([[1,2],[3,4]]))
display(df1.merge(df2))

# B.列名都相同，同名列存一个一个不相等值。			结果：空
df1 = pd.DataFrame(np.array([[1,2,3],[3,4,5]]))
df2 = pd.DataFrame(np.array([[1,2,4],[3,4,6]]))
display(df1.merge(df2))

# C.列名就存在不同。		结果：都加在一起
df1 = pd.DataFrame(np.array([[1,2,3],[3,4,5]]))
df2 = pd.DataFrame(np.array([[1,2,4],[3,4,6]]),columns=[0,1,3])
display(df1.merge(df2))

# how设置连接方式
df1 = pd.DataFrame(np.array([[1,2,3],[3,4,5]]))
df2 = pd.DataFrame(np.array([[1,2,4],[3,4,6]]))
# 默认为内连接（inner），连接不上时，则就不会再结果中显示
display(df1.merge(df2,how='inner'))
# 外连接(outer)，连接不上，2个DataFrame都会在结果中显示
display(df1.merge(df2,how='outer'))
# 左连接(left)，左边的DataFrame数据显示
display(df1.merge(df2,how='left'))
# 右连接，右边的DataFrame数据显示
display(df1.merge(df2,how='right'))

# on指定列作为连接条件，必须同时在两个DataFrame中出现
df1 = pd.DataFrame(np.array([[1,2,3],[3,4,5]]))
df2 = pd.DataFrame(np.array([[1,2,4],[3,4,6]]))
# 默认是根据相同列名作为连接条件
# 设置df1和df2中的0列作为连接条件（单一列）
display(df1.merge(df2,on=0))
# 设置Df1和Df2中0 1列作为连接条件（两列++）
display(df1.merge(df2,on=[0,1]))
# 不光符合条件会显示出来，不符合条件的也可以显示出来，只不过是_x和_y的区别。

# left_on right_on 指定2个DF的等值连接列名
df1 = pd.DataFrame(np.array([[1,2,3],[3,1,5]]))
df2 = pd.DataFrame(np.array([[2,1,4],[3,3,6]]))
display(df1.merge(df2,left_on=0,right_on=1))

# 可以通过left_index,right_index指定是否用行索引来充当连接条件。True，是；False，否
# 使用行索引作为等值连接条件，就不能指定列索引作为等值连接条件--》left_index(right_index)与left_on(right_on)不能同时指定
# Df1的行索引为0 1，df2行索引为0 1，值相等，数据可以连接
display(df1.merge(df2,left_index=True,right_index=True))

# suffixes自定义同名列后缀
# 2个Df连接后，存在同名列，默认同名列后缀_x _y,通过suffixes自定义后缀（_df1 _df2）
display(df1.merge(df2,left_index=True,right_index=True,suffixes=['_df1','_df2']))
```

#### join

​		与merge方法类似，但是**默认使用行索引进行连接**。

##### 常用参数（和merge用法全部一样）

- how：指定连接方式。可以使用Inner、outer、left、right，默认是left
- on：设置当前DataFrame使用哪个列与参数DataFrame对象的行索引进行连接
- isuffix/rsuffix：当两个DataFrame列名相同时，指定每个列名的后缀(用来区分),如果不指定，列名相同会产生错误

#### 三种连接方式对比（concat/merge/join）

1. concat：数据连接时，默认根据列索引来对齐，数据纵向连接
	1. concat axis = 1 根据行索引对齐，数据横向连接
2. merge和join，连接步骤都可以理解为两步：
	1. ​	数据水平组合在一起，（这一步merge和join相同）
	2. ​    根据等值连接条件显示
3. merge：默认根据所用的同名列的值，作为等值连接的条件。
4. join：默认根据行索引的索引值作为等值连接条件，且左连接显示